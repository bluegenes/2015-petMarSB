#!/usr/bin/env python
from __future__ import print_function

import argparse

from tasks import *

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resources-metadata', default='resources-test.json')
    parser.add_argument('--config-metadata', default='config-test.json')
    parser.add_argument('--print-tasks', action='store_true', default=False)
    parser.add_argument('--local-file-dir', default='_data')
    args, doit_args = parser.parse_known_args()

    with open(args.resources_metadata, 'r') as fp:
        print('** Using data resources found in {c}'.format(c=args.resources_metadata), file=sys.stderr)
        resources = json.load(fp)
    with open(args.config_metadata, 'r') as fp:
        print('** Using config found in {c}'.format(c=args.config_metadata), file=sys.stderr)
        config = json.load(fp)

    desc = '''
####################################################################
#
# 2015 Petromyzon marinus (sea lamprey) de novo RNA-seq Pipeline
#
# * Authors:
#   {authors}
#
# * About:
#   {desc}
#
####################################################################
'''.format(authors=', '.join(config['meta']['authors']),
           desc=config['meta']['description'])
    print(desc, file=sys.stderr)

    work_dir = config['pipeline']['work_dir']
    local_dir = os.path.abspath(args.local_file_dir)

    resources_df = pd.DataFrame(resources).transpose()
    sample_df = resources_df[resources_df.meta_type == 'sample']

    old_dir = os.getcwd()
    try:
        if not os.path.exists(work_dir):
            os.makedirs(work_dir)
        os.chdir(work_dir)
        print('** Current Working Directory: {w}'.format(w=os.getcwd()), file=sys.stderr)

        tasks = []

        '''
        Sample preprocessing
        '''

        link_tasks = sample_df.filename.apply(lambda fn: os.path.join(local_dir, fn)).apply(link_files_task)
        tasks.extend(list(link_tasks))
        tasks.append(group_task('link-samples', [t.name for t in link_tasks]))

        # Run diginorm in parallel on all samples
        ksize = config['pipeline']['khmer']['ksize']
        htsize = config['pipeline']['khmer']['parallel']['table_size']
        ntables = config['pipeline']['khmer']['parallel']['n_tables']
        coverage = config['pipeline']['khmer']['parallel']['coverage']
        dg_tasks = sample_df.apply(
                            lambda row: diginorm_task([row.filename], ksize,
                                htsize, coverage, ntables), axis=1, reduce=False)
        tasks.extend(list(dg_tasks))
        tasks.append(group_task('normalize-pass-1', [t.name for t in dg_tasks]))
        dg_outputs = sample_df.filename.apply(lambda fn: fn+'.keep')

        htsize = config['pipeline']['khmer']['pooled']['table_size']
        ntables = config['pipeline']['khmer']['pooled']['n_tables']
        coverage = config['pipeline']['khmer']['pooled']['coverage']
        ht_fn = config['pipeline']['prefix'] + '.pooled.ct'
        dg_task = diginorm_task(list(dg_outputs), ksize, htsize, coverage,
                                n_tables=ntables, ht_outfn=ht_fn)
        tasks.append(dg_task)
        tasks.append(group_task('normalize-pass-2', [dg_task.name]))
        dg_outputs = dg_outputs.apply(lambda fn: fn+'.keep')

        minabund = config['pipeline']['khmer']['parallel']['min_abund']
        abf_task = filter_abund_task(list(dg_outputs), ht_fn, minabund, coverage)
        tasks.append(group_task('filter-abund', [abf_task.name]))
        tasks.append(abf_task)

        tasks.append(group_task('preprocess', ['link-samples', 'normalize-pass-1', 'normalize-pass-2', 'filter-abund']))
        '''
        Database prep, homology search
        '''
        # Use curl to get remote flat files
        remote_files = resources_df[resources_df.access == 'remote_file']
        curl_tasks = list(remote_files.apply(lambda row: curl_task(row),
                                        axis=1, reduce=False))
        tasks.extend(curl_tasks)

        # Programmatically query uniprot
        uniprot_queries = resources_df[(resources_df.access == 'remote_query') & \
                                       (resources_df.q_type == 'uniprot')]
        uniprot_tasks = list(uniprot_queries.apply(lambda row: dict_to_task(
                                task_funcs.uniprot_query_task(
                                                        row.terms,
                                                        row.filename+'.gz')
                                ), axis=1, reduce=False))
        tasks.extend(uniprot_tasks)

        # unpack gzip'd remote flat files and queries
        gunzip_df = resources_df[(resources_df.access == 'remote_query') |
                                 (resources_df.access == 'remote_file')]
        gunzip_tasks = [dict_to_task(tsk) for tsk in
                      GunzipTask([(row.filename+'.gz', row.filename) for
                                   _, row in gunzip_df.iterrows()]).tasks()]
        tasks.extend(gunzip_tasks)

        hmmpress_tasks = [hmmpress_task(row.filename) for _, row in \
                   resources_df[resources_df.meta_type == 'hmm_profiles'].iterrows()]
        tasks.extend(hmmpress_tasks)

        # Truncate the long fasta names so blast doesn't choke
        truncate_tasks = list(gunzip_df[gunzip_df.meta_type == 'fasta_database'].apply(
                        lambda row: dict_to_task(task_funcs.truncate_fasta_header_task(row.filename)),
                        axis=1, reduce=False))
        tasks.extend(truncate_tasks)

        # Generate blast indices
        makeblastdb_tasks = list(resources_df[resources_df.meta_type.isin(['fasta_database', 'assembly'])].apply(
                        lambda row: dict_to_task(task_funcs.blast_format_task(
                            row.filename, row.filename+'.db', row.db_type)),
                            axis=1, reduce=False))
        tasks.extend(makeblastdb_tasks)

        tasks.append(group_task('databases', [t.name for t in curl_tasks + uniprot_tasks + \
                                              gunzip_tasks + truncate_tasks + \
                                              makeblastdb_tasks + hmmpress_tasks]))

        # Run blast
        blast_iters = []
        for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

            blast_iters.extend([blast_task(row, config, fn) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.meta_type != 'assembly')].iterrows()])

        blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

        blast_tasks = []
        for tskiter in blast_iters:
            blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
        tasks.extend(blast_tasks)
        tasks.append(group_task('blast', [t.name for t in blast_tasks]))

        # Run pfam search
        dbfn = resources_df.ix['pfamA'].filename
        hmmscan_tasks = []
        for fn in resources_df[resources_df.meta_type == 'assembly'].filename:
            hmmscan_tasks.append(hmmscan_task(fn, fn + '.pfam-A.out', dbfn, 
                                          config['pipeline']['hmmscan']))
        tasks.extend(hmmscan_tasks)
        tasks.append(group_task('hmmscan', [t.name for t in hmmscan_tasks]))

        '''
        BUSCO
        '''

        busco_cfg = config['pipeline']['busco']
        busco_tasks = []
        busco_db_task = download_and_untar_task(busco_cfg['vert_url'], busco_cfg['db_dir'])
        busco_tasks.append(busco_db_task)

        for assembly in list(resources_df[resources_df.meta_type == 'assembly'].filename):
            output_dir = strip_seq_extension(assembly) + busco_cfg['output_suffix']
            busco_tasks.append(busco_task(assembly, output_dir, 
                       os.path.join(busco_cfg['db_dir'], 'vertebrata'),
                       'trans', busco_cfg))

        tasks.extend(busco_tasks)
        tasks.append(group_task('busco', [t.name for t in busco_tasks]))

        '''
        Alignment / abundance estimation stuff
        '''
        bt_build_tasks = list(resources_df[resources_df.meta_type == 'assembly'].apply(bowtie2_build_task, axis=1, reduce=False))
        index_basenames = resources_df[resources_df.meta_type == 'assembly'].filename.apply(lambda fn: fn.rstrip('.fasta'))
        tasks.extend(bt_build_tasks)
        tasks.append(group_task('bowtie-build', [t.name for t in bt_build_tasks]))

        split_tasks = sample_df[sample_df.paired == True].apply(split_pairs_task, axis=1, reduce=False)
        tasks.extend(list(split_tasks))

        estimate_groups = []
        for assembly in list(resources_df[resources_df.meta_type == 'assembly'].filename):
            index_fn = assembly.rstrip('.fasta')
            align_tasks = list(sample_df.apply(bowtie2_align_task, args=(index_fn,config['pipeline']['bowtie']['n_threads']), axis=1, reduce=False))
            tasks.extend(align_tasks)
            tasks.append(group_task('align-samples-'+index_fn, [t.name for t in align_tasks]))

            align_files_df = task_funcs.get_task_files_df(align_tasks)
            bam_files_to_sort = align_files_df[align_files_df.apply(lambda row: row.dep.endswith('.1') and row.target.endswith('.bam'), axis=1)].target
            bam_files_single = align_files_df[align_files_df.apply(lambda row: row.dep.endswith('.fq.gz') and row.target.endswith('.bam'), axis=1)].target
            sort_tasks = bam_files_to_sort.apply(samtools_sort_task)
            tasks.extend(sort_tasks)
            tasks.append(group_task('sort-alignments-' + index_fn, [t.name for t in sort_tasks]))

            hits_files = bam_files_single.append(sort_tasks.apply(lambda t: t.targets[0]))
            express_tasks = list(hits_files.apply(express_task, args=(assembly,)))
            tasks.extend(express_tasks)
            tasks.append(group_task('express-'+index_fn, [t.name for t in express_tasks]))

            # The results.xprs file is always in position 1 in the targets --
            # kludgy, fix this later...
            express_files = [t.targets[1] for t in express_tasks]
            agg_task = aggregate_express_task(express_files, '{pref}.eXpress.tpm.tsv'.format(pref=index_fn),
                                                         '{pref}.eXpress.eff.tsv'.format(pref=index_fn),
                                                         '{pref}.eXpress.tot.tsv'.format(pref=index_fn))
            tasks.append(agg_task)
            tasks.append(group_task('estimate-abundance-'+index_fn, [agg_task.name]))
            estimate_groups.append('estimate-abundance-'+index_fn)
        tasks.append(group_task('estimate-abundance', estimate_groups))

        if args.print_tasks:
            for task in tasks:
                print('-----\n', task)
                pprint.pprint(task.__dict__)

        task_funcs.run_tasks(tasks, doit_args)

    finally:
        os.chdir(old_dir)

main()
