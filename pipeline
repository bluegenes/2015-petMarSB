#!/usr/bin/env python
from __future__ import print_function

import argparse
from annotate import *
from tasks import *

def get_abundance_estimation_tasks(assembly_fn, sample_df, config):
    
    tasks = []

    '''
    The first task is to build the bowtie2 index for the assembly.
    We'll generate the basename from the assembly filename, then give it
    to the task creator.
    '''
    bt2_db_basename = strip_seq_extension(assembly_fn) + '.bt2idx'
    bt2_build_cfg = config['pipeline']['bowtie2_build']
    bt2_build_task = bowtie2_build_task(assembly_fn, bt2_db_basename, bt2_build_cfg)
    tasks.append(bt2_build_task)

    #split_tasks = sample_df[sample_df.paired == True].apply(split_pairs_task, axis=1, reduce=False)
    #tasks.extend(list(split_tasks))

    '''
    Now we generate the align and eXpress tasks for each sample
    '''
    for key, row in sample_df.iterrows():
        target_fn = '{sample}.x.{idx}'.format(sample=row.filename, idx=bt2_db_basename)
        encoding = row.encoding
        sample_fn = row.filename
        bowtie2_align_cfg = config['pipeline']['bowtie2_build']

        if row.paired == True:
            align_task = bowtie2_align_task(bt2_db_basename, target_fn, bowtie2_align_cfg,
                                            left_fn=sample_fn+'.1', right_fn=sample_fn+'.2',
                                            singleton_fn=sample_fn+'.0', encoding=encoding)
            tasks.append(align_task)
            # We have to sort results with samtools when we have paired input
            sort_task = samtools_sort_task(target_fn + '.bam')
            tasks.append(sort_task)
            # The final result has a rather verbose extension
            bam_fn = target_fn + '.bam.sorted.bam'
        else:
            align_task = bowtie2_align_task(bt2_db_basename, target_fn, bowtie2_align_cfg,
                                            singleton_fn=sample_fn, encoding=encoding)
            tasks.append(align_task)
            bam_fn = target_fn + '.bam'
        
        # Generate directory name for eXpress results
        express_dir = bam_fn.split('.bam')[0]
        express_task = eXpress_task(assembly_fn, bam_fn, express_dir)
        tasks.append(express_task)

        # Get the eXpress results file
        express_results = os.path.join(express_dir, 'results.xprs')
        # Aggregate them
        agg_task = aggregate_express_task(express_files,
                                          '{pref}.eXpress.tpm.tsv'.format(pref=assembly_fn),
                                          '{pref}.eXpress.eff.tsv'.format(pref=assembly_fn),
                                          '{pref}.eXpress.tot.tsv'.format(pref=assembly_fn))
        tasks.append(agg_task)
    
    return tasks

def get_sample_prep_tasks(sample_df, config, data_dir):
    tasks = []
    '''
    First, we link all the files into the current directory.
    '''
    for key, row in sample_df.iterrows():
        tasks.append(link_file_task(os.path.join(data_dir, row.filename)))

    

    # Run diginorm in parallel on all samples
    ksize = config['pipeline']['khmer']['ksize']
    htsize = config['pipeline']['khmer']['parallel']['table_size']
    ntables = config['pipeline']['khmer']['parallel']['n_tables']
    coverage = config['pipeline']['khmer']['parallel']['coverage']
    dg_tasks = sample_df.apply(
                        lambda row: diginorm_task([row.filename], ksize,
                            htsize, coverage, ntables), axis=1, reduce=False)
    tasks.extend(list(dg_tasks))
    tasks.append(group_task('normalize-pass-1', [t.name for t in dg_tasks]))
    dg_outputs = sample_df.filename.apply(lambda fn: fn+'.keep')

    htsize = config['pipeline']['khmer']['pooled']['table_size']
    ntables = config['pipeline']['khmer']['pooled']['n_tables']
    coverage = config['pipeline']['khmer']['pooled']['coverage']
    ht_fn = config['pipeline']['prefix'] + '.pooled.ct'
    dg_task = diginorm_task(list(dg_outputs), ksize, htsize, coverage,
                            n_tables=ntables, ht_outfn=ht_fn)
    tasks.append(dg_task)
    tasks.append(group_task('normalize-pass-2', [dg_task.name]))
    dg_outputs = dg_outputs.apply(lambda fn: fn+'.keep')

    minabund = config['pipeline']['khmer']['parallel']['min_abund']
    abf_task = filter_abund_task(list(dg_outputs), ht_fn, minabund, coverage)
    tasks.append(group_task('filter-abund', [abf_task.name]))
    tasks.append(abf_task)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--resources-metadata', default='resources.json')
    parser.add_argument('--config-metadata', default='config.json')
    parser.add_argument('--print-tasks', action='store_true', default=False)
    parser.add_argument('--local-file-dir', default='_data')
    parser.add_argument('--assembly-file', default='lamp10.fasta')
    args, doit_args = parser.parse_known_args()

    with open(args.resources_metadata, 'r') as fp:
        print('** Using data resources found in {c}'.format(c=args.resources_metadata), file=sys.stderr)
        resources = json.load(fp)
    with open(args.config_metadata, 'r') as fp:
        print('** Using config found in {c}'.format(c=args.config_metadata), file=sys.stderr)
        config = json.load(fp)

    desc = '''
####################################################################
#
# 2015 Petromyzon marinus (sea lamprey) de novo RNA-seq Pipeline
#
# * Authors:
#   {authors}
#
# * About:
#   {desc}
#
####################################################################
'''.format(authors=', '.join(config['meta']['authors']),
           desc=config['meta']['description'])
    print(desc, file=sys.stderr)

    assembly_fn = args.assembly_file

    work_dir = config['pipeline']['work_dir']
    local_dir = os.path.abspath(args.local_file_dir)

    resources_df = pd.DataFrame(resources).transpose()
    sample_df = resources_df[resources_df.meta_type == 'sample']

    old_dir = os.getcwd()
    try:
        if not os.path.exists(work_dir):
            os.makedirs(work_dir)
        os.chdir(work_dir)
        print('** Current Working Directory: {w}'.format(w=os.getcwd()), file=sys.stderr)

        tasks = []

        '''
        Sample preprocessing
        '''

 
        tasks.append(group_task('preprocess', ['link-samples', 'normalize-pass-1', 'normalize-pass-2', 'filter-abund']))
        '''
        Database prep, homology search
        '''
        # Use curl to get remote flat files
        remote_files = resources_df[resources_df.access == 'remote_file']
        curl_tasks = list(remote_files.apply(lambda row: curl_task(row),
                                        axis=1, reduce=False))
        tasks.extend(curl_tasks)

        # Programmatically query uniprot
        uniprot_queries = resources_df[(resources_df.access == 'remote_query') & \
                                       (resources_df.q_type == 'uniprot')]
        uniprot_tasks = list(uniprot_queries.apply(lambda row: dict_to_task(
                                task_funcs.uniprot_query_task(
                                                        row.terms,
                                                        row.filename+'.gz')
                                ), axis=1, reduce=False))
        tasks.extend(uniprot_tasks)

        # unpack gzip'd remote flat files and queries
        gunzip_df = resources_df[(resources_df.access == 'remote_query') |
                                 (resources_df.access == 'remote_file')]
        gunzip_tasks = [dict_to_task(tsk) for tsk in
                      GunzipTask([(row.filename+'.gz', row.filename) for
                                   _, row in gunzip_df.iterrows()]).tasks()]
        tasks.extend(gunzip_tasks)

        hmmpress_tasks = [hmmpress_task(row.filename) for _, row in \
                   resources_df[resources_df.meta_type == 'hmm_profiles'].iterrows()]
        tasks.extend(hmmpress_tasks)

        # Truncate the long fasta names so blast doesn't choke
        truncate_tasks = list(gunzip_df[gunzip_df.meta_type == 'fasta_database'].apply(
                        lambda row: dict_to_task(task_funcs.truncate_fasta_header_task(row.filename)),
                        axis=1, reduce=False))
        tasks.extend(truncate_tasks)

        # Generate blast indices
        makeblastdb_tasks = list(resources_df[resources_df.meta_type.isin(['fasta_database', 'assembly'])].apply(
                        lambda row: dict_to_task(task_funcs.blast_format_task(
                            row.filename, row.filename+'.db', row.db_type)),
                            axis=1, reduce=False))
        tasks.extend(makeblastdb_tasks)

        tasks.append(group_task('databases', [t.name for t in curl_tasks + uniprot_tasks + \
                                              gunzip_tasks + truncate_tasks + \
                                              makeblastdb_tasks + hmmpress_tasks]))

        # Run blast
        blast_iters = []
        for fn in resources_df[resources_df.meta_type == 'assembly'].filename:

            blast_iters.extend([blast_task(row, config, fn) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.meta_type != 'assembly')].iterrows()])

        blast_iters.extend([blast_task(row, config, resources_df.ix['petMar2_cdna'].filename) \
                        for _, row in resources_df[(resources_df.meta_type == 'fasta_database') &
                                                   (resources_df.filename != 'petMar2.cdna.fa')].iterrows()])

        blast_tasks = []
        for tskiter in blast_iters:
            blast_tasks.extend([dict_to_task(tsk) for tsk in tskiter])
        tasks.extend(blast_tasks)
        tasks.append(group_task('blast', [t.name for t in blast_tasks]))

        '''
        TransDecoder and hmmscan
        '''
        tdc_tasks = []
        dbfn = resources_df.ix['pfamA'].filename
        for fn in resources_df[resources_df.meta_type == 'assembly'].filename:
            tdc_tasks.append(transdecoder_orf_task(fn, config['pipeline']['transdecoder']))
            pep_fn = os.path.join(fn+'.transdecoder_dir', 'longest_orfs.pep')
            tdc_tasks.append(hmmscan_task(pep_fn, fn + '.pfam-A.out', dbfn, 
                                          config['pipeline']['hmmscan']))
            tdc_tasks.append(transdecoder_predict_task(fn, fn + '.pfam-A.out',
                            config['pipeline']['transdecoder']))
        tasks.extend(tdc_tasks)
        tasks.append(group_task('transdecoder', [t.name for t in tdc_tasks]))

        '''
        BUSCO
        '''

        busco_cfg = config['pipeline']['busco']
        busco_tasks = []
        busco_db_task = download_and_untar_task(busco_cfg['vert_url'], busco_cfg['db_dir'])
        busco_tasks.append(busco_db_task)

        for assembly in list(resources_df[resources_df.meta_type == 'assembly'].filename):
            output_dir = strip_seq_extension(assembly) + busco_cfg['output_suffix']
            busco_tasks.append(busco_task(assembly, output_dir, 
                       os.path.join(busco_cfg['db_dir'], 'vertebrata'),
                       'trans', busco_cfg))

        tasks.extend(busco_tasks)
        tasks.append(group_task('busco', [t.name for t in busco_tasks]))

        abundance_tasks = get_abundance_estimation(assembly_fn, sample_df, config)
        tasks.extend(abundance_tasks)
        tasks.append(group_task('abundance', [t.name for t in abundance_tasks]))


        blast_targets = list(resources_df[(resources_df.meta_type == 'fasta_database') &
                                          (resources_df.meta_type != 'assembly')].filename)
        for assembly in list(resources_df[resources_df.meta_type == 'assembly'].filename):
            ann_task = aggregate_annotations_task(assembly, blast_targets, 
                                                  assembly+'.transdecoder.gff3',
                                                  assembly+'.pfam-A.out', sample_df,
                                                  strip_seq_extension(assembly)+'.eXpress.tpm.tsv',
                                                  assembly+'.annotations.h5')
            tasks.append(ann_task)


        if args.print_tasks:
            for task in tasks:
                print('-----\n', task)
                pprint.pprint(task.__dict__)

        if doit_args:
            task_funcs.run_tasks(tasks, doit_args)

    finally:
        os.chdir(old_dir)

main()
